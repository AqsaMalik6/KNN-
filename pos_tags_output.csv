text_column,tokenized_text,normalized_text,stemmed_text,pos_tags
This is an example sentence.,"['This', 'is', 'an', 'example', 'sentence', '.']","['this', 'is', 'an', 'example', 'sentence', '.']","['thi', 'is', 'an', 'exampl', 'sentenc', '.']","[('this', 'PRON'), ('is', 'AUX'), ('an', 'DET'), ('example', 'NOUN'), ('sentence', 'NOUN'), ('.', 'PUNCT')]"
Tokenization is an important step in NLP.,"['Tokenization', 'is', 'an', 'important', 'step', 'in', 'NLP', '.']","['tokenization', 'is', 'an', 'important', 'step', 'in', 'nlp', '.']","['token', 'is', 'an', 'import', 'step', 'in', 'nlp', '.']","[('tokenization', 'NOUN'), ('is', 'AUX'), ('an', 'DET'), ('important', 'ADJ'), ('step', 'NOUN'), ('in', 'ADP'), ('nlp', 'NOUN'), ('.', 'PUNCT')]"
Let's test the text preprocessing script.,"['Let', ""'s"", 'test', 'the', 'text', 'preprocessing', 'script', '.']","['let', ""'s"", 'test', 'the', 'text', 'preprocessing', 'script', '.']","['let', ""'s"", 'test', 'the', 'text', 'preprocess', 'script', '.']","[('let', 'VERB'), (""'s"", 'PRON'), ('test', 'VERB'), ('the', 'DET'), ('text', 'NOUN'), ('preprocessing', 'VERB'), ('script', 'NOUN'), ('.', 'PUNCT')]"
